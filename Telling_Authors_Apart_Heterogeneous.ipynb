{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis\n",
    "Analyzes texts from seemingly-similar authors, Rachel Swirsky, Cat Valente, and Carmen Maria Machado, on a sentence-by-sentence basis, to see if their writing styles can be predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "from urllib.request import Request, urlopen\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from time import time\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "pd.set_option('display.max_columns', 100)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Data\n",
    "\n",
    "### Requesting and parsing html\n",
    "* Using requests and BeautifulSoup\n",
    "* The program first pulls stories directly from their published websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get raw in 21.51 seconds\n"
     ]
    }
   ],
   "source": [
    "# Pull stories from their websites\n",
    "\n",
    "header={'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) \\\n",
    "        AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.139 Safari/537.36'}\n",
    "\n",
    "def get_raw(url, no_strong=False, no_b=False, no_emoji=False, no_small_caps=False):\n",
    "    response = requests.get(url, headers=header)\n",
    "    html = response.content\n",
    "    soup = BeautifulSoup(html, 'lxml-xml')\n",
    "    if no_strong == True:\n",
    "        for strong in soup('strong'):\n",
    "            strong.decompose()\n",
    "    if no_b == True:\n",
    "        for b in soup('b'):\n",
    "            b.decompose()\n",
    "    if no_emoji == True:\n",
    "        for emoji in soup.find_all('img', class_='emoji'):\n",
    "            emoji.decompose()\n",
    "    if no_small_caps == True:\n",
    "        for caps in soup.find_all('p', {'class':'small-caps'}):\n",
    "            caps.decompose()\n",
    "    return soup.get_text()\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "# Swirsky stories\n",
    "dino_raw = get_raw('https://www.apex-magazine.com/if-you-were-a-dinosaur-my-love/')\n",
    "eros_raw = get_raw('https://www.tor.com/2009/03/03/eros-philia-agape/')\n",
    "still_raw = get_raw('http://uncannymagazine.com/article/love-is-never-still/', no_b=True)\n",
    "\n",
    "# Valente stories\n",
    "fade_raw = get_raw('http://clarkesworldmagazine.com/valente_08_12/')\n",
    "silent1_raw = get_raw('http://clarkesworldmagazine.com/valente_10_11/', no_strong=True)\n",
    "silent2_raw = get_raw('http://clarkesworldmagazine.com/valente_11_11/', no_strong=True)\n",
    "silent3_raw = get_raw('http://clarkesworldmagazine.com/valente_12_11/', no_strong=True)\n",
    "lion_raw = get_raw('http://uncannymagazine.com/article/planet-lion/')\n",
    "\n",
    "# Machado stories\n",
    "stitch_raw = get_raw('https://granta.com/The-Husband-Stitch/')\n",
    "follow_raw = get_raw('http://www.lightspeedmagazine.com/fiction/help-follow-sister-land-dead/')\n",
    "descent_raw = get_raw('http://www.nightmare-magazine.com/fiction/descent/')\n",
    "bites_raw = get_raw('http://texas.gulfcoastmag.org/journal/29.2-summer/fall-2017/eight-bites/')\n",
    "body_raw = get_raw('http://uncannymagazine.com/article/my-body-herself/')\n",
    "heinous_raw = get_raw('http://theamericanreader.com/especially-heinous-272-views-of-law-order-svu/', no_emoji=True, no_small_caps=True)\n",
    "\n",
    "print('Get raw in {:.2f} seconds'.format(time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Especially Heinous: 272 Views of Law & Order SVU | The American Reader\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\twindow._wpemojiSettings = {\"baseUrl\":\"https:\\/\\/s.w.org\\/images\\/core\\/emoji\\/2.3\\/72x72\\/\",\"ext\":\".png\",\"svgUrl\":\"https:\\/\\/s.w.org\\/images\\/core\\/emoji\\/2.3\\/svg\\/\",\"svgExt\":\".svg\",\"source\":{\"concatemoji\":\"\\/wp-includes\\/js\\/wp-emoji-release.min.js?ver=4.8.6\"}};\n",
      "\t\t\t!function(a,b,c){function d(a){var b,c,d,e,f=String.fromCharCode;if(!k||!k.fillText)return!1;switch(k.clearRect(0,0,j.width,j.height),k.textBaseline=\"top\",k.font=\"600 32px Arial\",a){case\"flag\":return k.fillText(f(55356,56826,55356,56819),0,0),b=j.toDataURL(),k.clearRect(0,0,j.width,j.height),k.fillText(f(55356,56826,8203,55356,56819),0,0),c=j.toDataURL(),b!==c(k.clearRect(0,0,j.width,j.height),k.fillText(f(55356,57332,56128,56423,56128,56418,56128,56421,56128,56430,56128,56423,56128,56447),0,0),b=j.toDataURL(),k.clearRect(0,0,j.width,j.height),k.fillText(f(55356,57332,8203,56128,56423,8203,56128,56418,8203,56128,56421,8203,56128,56430,8203,56128,56423,8203,56128,56447),0,0),c=j.toDataURL(),b!==c);case\"emoji4\":return k.fillText(f(55358,56794,8205,9794,65039),0,0),d=j.toDataURL(),k.clearRect(0,0,j.width,j.height),k.fillText(f(55358,56794,8203,9794,65039),0,0),e=j.toDataURL(),d!==e}return!1}function e(a){var c=b.createElement(\"script\");c.src=a,c.defer=c.type=\"text/javascript\",b.getElementsByTagName(\"head\")[0].appendChild(c)}var f,g,h,i,j=b.createElement(\"canvas\"),k=j.getContext(\"2d\");for(i=Array(\"flag\",\"emoji4\"),c.supports={everything:!0,everythingExceptFlag:!0},h=0;h;h++)c.supports[i[h]]=d(i[h]),c.supports.everything=c.supports.everything[i[h]],\"flag\"!==i[h](c.supports.everythingExceptFlag=c.supports.everythingExceptFlag[i[h]]);c.supports.everythingExceptFlag=c.supports.everythingExceptFlag!c.supports.flag,c.DOMReady=!1,c.readyCallback=function(){c.DOMReady=!0},c.supports.everything||(g=function(){c.readyCallback()},b.addEventListener?(b.addEventListener(\"DOMContentLoaded\",g,!1),a.addEventListener(\"load\",g,!1)):(a.attachEvent(\"onload\",g),b.attachEvent(\"onreadystatechange\",function(){\"complete\"===b.readyState()})),f=c.source||{},f.concatemoji?e(f.concatemoji):f.wpemoji(e(f.twemoji),e(f.wpemoji)))}(window,document,window._wpemojiSettings);\n",
      "\t\t\n",
      "\n",
      "img.wp-smiley,\n",
      "img.emoji {\n",
      "\tdisplay: inline !important;\n",
      "\tborder: none !important;\n",
      "\tbox-shadow: none !important;\n",
      "\theight: 1em !important;\n",
      "\twidth: 1em !important;\n",
      "\tmargin: 0 .07em !important;\n",
      "\tvertical-align: -0.1em !important;\n",
      "\tbackground: none !important;\n",
      "\tpadding: 0 !important;\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "window.jQuery || document.write('\n"
     ]
    }
   ],
   "source": [
    "print(heinous_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding story text in html\n",
    "* Using find() from BeautifulSoup\n",
    "* Identifies story text inside the total site text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select out story text\n",
    "\n",
    "def select_out(raw, start_text, end_text):\n",
    "    start = raw.find(start_text)\n",
    "    end = raw.find(end_text)\n",
    "    return raw[start:end+len(end_text)]\n",
    "\n",
    "dinosaur = select_out(dino_raw, \n",
    "                      'If you were a dinosaur, my', \n",
    "                      ', and the stuttering of my broken heart.'\n",
    "                     )\n",
    "eros = select_out(eros_raw, \n",
    "                  'Lucian packed his possessions before he left.', \n",
    "                  'yet form the thoughts to wonder what will happen next.\\nHe moves on.'\n",
    "                 )\n",
    "still = select_out(still_raw, \n",
    "                  'Through every moment of carving, I want her as one wants a woman. I want', \n",
    "                  'and decay, but it is also the early bloom that opens into winterâ€™s cold. She is love, and she will always be reborn.'\n",
    "                  )\n",
    "fade = select_out(fade_raw, \n",
    "                 'ZOOM IN on a bright-eyed Betty in a crisp green dress,', \n",
    "                 'And over the black, a cheerful fat man giving the thumbs up to Sylvie, grinning:\\n\\nBuy Freedom Brand Film! It\\'s A-OK!'\n",
    "                 )\n",
    "silent1 = select_out(silent1_raw, \n",
    "                    'Inanna was called Queen of Heaven and Earth,', \n",
    "                    'The sun breaks the mountain crests, hard and cold, a shaft of white spilling over the black lake.'\n",
    "                    )\n",
    "silent2 = select_out(silent2_raw, \n",
    "                    'Humanity lived many years and ruled the earth,', \n",
    "                    'The castle windows go dark, one by one.'\n",
    "                    )\n",
    "silent3 = select_out(silent3_raw, \n",
    "                    'Tell me a story about yourself, Elefsis.', \n",
    "                    'we walk up the long path out of the churning, honey-colored sea.'\n",
    "                    )\n",
    "silent = silent1 + '\\n' + silent2 + '\\n' + silent3\n",
    "\n",
    "lion = select_out(lion_raw, \n",
    "                 'Initial Survey Report: Planet 6MQ441 (Bakeneko), Alaraph', \n",
    "                 'Szent Istvan. She longs to hear the first roar of her young.'\n",
    "                 )\n",
    "stitch = select_out(stitch_raw, \n",
    "                   '(If you read this story out loud, please use', \n",
    "                   'backwards off my neck and rolls off the bed, I feel as lonely as I have ever been.'\n",
    "                   )\n",
    "follow = select_out(follow_raw, \n",
    "                   'Help Me Follow My Sister\\ninto the Land of the Dead\\nby Ursula Ruiz', \n",
    "                   'sâ€”in a terrifying place where no mortal has any business treading.'\n",
    "                   ) + '\\n' + select_out(follow_raw, \n",
    "                                        'Home\\nThis is the thing about my sister and I:', \n",
    "                                        'Lucille L\\nJun 28, 2015\\nUrsula?'\n",
    "                                        )\n",
    "descent = select_out(descent_raw, \n",
    "                    'We gathered for the last time in October,', \n",
    "                    ', drinks cocked in their hands.\\nI looked down.'\n",
    "                    )\n",
    "bites = select_out(bites_raw, \n",
    "                  'As they put me to sleep, my mouth fills with the dust of the moon. I expect to choke on the silt but', \n",
    "                  'my charge.\\\"I\\'m sorry,\\\" I will whisper into her as she walks me toward the front door.\\\"I\\'m sorry,\\\" I will repeat. \\\"I didn\\'t know.\\\"')\n",
    "body = select_out(body_raw, \n",
    "                 'When the caveâ€™s ceiling crumples, so do I. Through my body,', \n",
    "                 'the privilege to see: a young woman, born screaming.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing Text\n",
    "\n",
    "### Cleaning \n",
    "* Cleaning out un-parseable symbols\n",
    "* Using re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility function for standard text cleaning.\n",
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(r'[\\[\\]]', '', text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clean out bad symbols\n",
    "dinosaur = text_cleaner(dinosaur)\n",
    "eros = text_cleaner(eros)\n",
    "fade = text_cleaner(fade)\n",
    "silent = text_cleaner(silent)\n",
    "lion = text_cleaner(lion)\n",
    "still = text_cleaner(still)\n",
    "stitch = text_cleaner(stitch)\n",
    "follow = text_cleaner(follow)\n",
    "descent = text_cleaner(descent)\n",
    "bites = text_cleaner(bites)\n",
    "body = text_cleaner(body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing documents\n",
    "* Using spaCy\n",
    "* Identifies sentence structure in the stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed in 25.00 seconds\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "\n",
    "# Parse using spaCy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "dinosaur_doc = nlp(dinosaur)\n",
    "eros_doc = nlp(eros)\n",
    "fade_doc = nlp(fade)\n",
    "silent_doc = nlp(silent)\n",
    "lion_doc = nlp(lion)\n",
    "still_doc = nlp(still)\n",
    "stitch_doc = nlp(stitch)\n",
    "follow_doc = nlp(follow)\n",
    "descent_doc = nlp(descent)\n",
    "bites_doc = nlp(bites)\n",
    "body_doc = nlp(body)\n",
    "\n",
    "print('Parsed in {:.2f} seconds'.format(time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping data into sentences\n",
    "* We use spaCy's tokenization and sentence identification\n",
    "* Creates one table of all the data, organized with one sentence for each line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Group sentences with their authors. (Each sentence will be a row.)\n",
    "\n",
    "dinosaur_sents = [[\"Swirsky\", sent.string] for sent in dinosaur_doc.sents]\n",
    "eros_sents = [[\"Swirsky\", sent.string] for sent in eros_doc.sents]\n",
    "still_sents = [[\"Swirsky\", sent.string] for sent in still_doc.sents]\n",
    "fade_sents = [[\"Valente\", sent.string] for sent in fade_doc.sents]\n",
    "silent_sents = [[\"Valente\", sent.string] for sent in silent_doc.sents]\n",
    "lion_sents = [[\"Valente\", sent.string] for sent in lion_doc.sents]\n",
    "stitch_sents = [[\"Machado\", sent.string] for sent in stitch_doc.sents]\n",
    "follow_sents = [[\"Machado\", sent.string] for sent in follow_doc.sents]\n",
    "descent_sents = [[\"Machado\", sent.string] for sent in descent_doc.sents]\n",
    "bites_sents = [[\"Machado\", sent.string] for sent in bites_doc.sents]\n",
    "body_sents = [[\"Machado\", sent.string] for sent in body_doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Take part of speech of each word in each sentence.\n",
    "\n",
    "dinosaur_pos = [[token.pos_ for token in sent] for sent in dinosaur_doc.sents]\n",
    "eros_pos = [[token.pos_ for token in sent] for sent in eros_doc.sents]\n",
    "still_pos = [[token.pos_ for token in sent] for sent in still_doc.sents]\n",
    "fade_pos = [[token.pos_ for token in sent] for sent in fade_doc.sents]\n",
    "silent_pos = [[token.pos_ for token in sent] for sent in silent_doc.sents]\n",
    "lion_pos = [[token.pos_ for token in sent] for sent in lion_doc.sents]\n",
    "stitch_pos = [[token.pos_ for token in sent] for sent in stitch_doc.sents]\n",
    "follow_pos = [[token.pos_ for token in sent] for sent in follow_doc.sents]\n",
    "descent_pos = [[token.pos_ for token in sent] for sent in descent_doc.sents]\n",
    "bites_pos = [[token.pos_ for token in sent] for sent in bites_doc.sents]\n",
    "body_pos = [[token.pos_ for token in sent] for sent in body_doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# list(dinosaur_doc.print_tree())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Data Set:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Valente</th>\n",
       "      <td>2269</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Swirsky</th>\n",
       "      <td>1880</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Machado</th>\n",
       "      <td>1758</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         count  ratio\n",
       "Valente   2269   0.38\n",
       "Swirsky   1880   0.32\n",
       "Machado   1758   0.30"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5907 total sentences\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>words</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Swirsky</td>\n",
       "      <td>if you were a dinosaur  my love  then you woul...</td>\n",
       "      <td>14</td>\n",
       "      <td>[ADP, PRON, VERB, DET, NOUN, PUNCT, ADJ, NOUN,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    author                                               text  words  \\\n",
       "0  Swirsky  if you were a dinosaur  my love  then you woul...     14   \n",
       "\n",
       "                                                 pos  \n",
       "0  [ADP, PRON, VERB, DET, NOUN, PUNCT, ADJ, NOUN,...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>words</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2153</th>\n",
       "      <td>Valente</td>\n",
       "      <td>was called queen of heaven and earth  queen o...</td>\n",
       "      <td>37</td>\n",
       "      <td>[PROPN, VERB, VERB, PROPN, ADP, PROPN, CCONJ, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       author                                               text  words  \\\n",
       "2153  Valente   was called queen of heaven and earth  queen o...     37   \n",
       "\n",
       "                                                    pos  \n",
       "2153  [PROPN, VERB, VERB, PROPN, ADP, PROPN, CCONJ, ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>words</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4486</th>\n",
       "      <td>Machado</td>\n",
       "      <td>if you read this story out loud  please use t...</td>\n",
       "      <td>24</td>\n",
       "      <td>[PUNCT, ADP, PRON, VERB, DET, NOUN, ADV, ADV, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       author                                               text  words  \\\n",
       "4486  Machado   if you read this story out loud  please use t...     24   \n",
       "\n",
       "                                                    pos  \n",
       "4486  [PUNCT, ADP, PRON, VERB, DET, NOUN, ADV, ADV, ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Combine the sentences from the stories into one data frame.\n",
    "sentences = pd.DataFrame(dinosaur_sents + eros_sents + still_sents + fade_sents + silent_sents + lion_sents + stitch_sents + follow_sents + descent_sents + bites_sents + body_sents)\n",
    "sentences.columns = ['author', 'text']\n",
    "\n",
    "# Strip out unnecessary punctuation.\n",
    "sentences['text'] = sentences['text'].str.lower().str.replace(r'[^a-zA-Z0-9 ]+', ' ').fillna('')\n",
    "\n",
    "# Word count per sentence.\n",
    "sentences['words'] = sentences['text'].str.split().apply(len)\n",
    "\n",
    "# Parts of speech classification of each word in sentence.\n",
    "sentences['pos'] = pd.Series(dinosaur_pos + eros_pos + still_pos + fade_pos + silent_pos + lion_pos + stitch_pos + follow_pos + descent_pos + bites_pos + body_pos)\n",
    "\n",
    "# Remove blank sentences.\n",
    "sentences = sentences.loc[sentences['words'] > 0]\n",
    "\n",
    "# Remove proper nouns personal to each document.\n",
    "propn_pat = r'\\b(agogna|koetoi|adriana|lucian|galatea|aphrodite|hephaestus|fuoco|inanna|enki|erishkigal|salma|janet|lawrence|olive|luna|ursula|ceno|neva|elefsis|ravan|cassian|diane|ares|ben|seki|ilet)\\b'\n",
    "sentences['text'] = sentences['text'].str.replace(propn_pat, '')\n",
    "\n",
    "# Sentence counts by author.\n",
    "df_count = sentences.author.value_counts().to_frame(name='count')\n",
    "df_count['ratio'] = (df_count['count'] / len(sentences.author)).round(2)\n",
    "print('Full Data Set:')\n",
    "display(df_count)\n",
    "print('{} total sentences'.format(len(sentences.author)))\n",
    "display(sentences.loc[sentences['author'] == 'Swirsky'].head(1))\n",
    "display(sentences.loc[sentences['author'] == 'Valente'].head(1))\n",
    "display(sentences.loc[sentences['author'] == 'Machado'].head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Test Set\n",
    "This protects the vectorizer and regressor from over-fitting to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(sentences.drop(['author', 'words'], axis=1), sentences['author'], test_size=0.2)\n",
    "X_train = sentences.drop(['author', 'words'], axis=1)\n",
    "y_train = sentences['author']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set:\n",
      "         count\n",
      "Valente   1842\n",
      "Swirsky   1496\n",
      "Machado   1263\n",
      "\n",
      "Test Set:\n",
      "         count\n",
      "Valente    427\n",
      "Swirsky    384\n",
      "Machado    340\n"
     ]
    }
   ],
   "source": [
    "print('Training Set:')\n",
    "print(y_train.value_counts().to_frame(name='count'))\n",
    "print('\\nTest Set:')\n",
    "print(y_test.value_counts().to_frame(name='count'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline: Selector, Transformers and Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item Selector\n",
    "* Makes it possible to run different transformers on different columns of data before concatenating them together with FeatureUnion and then running through the estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "# The key corresponding to the desired value in a mappable.\n",
    "\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF Vectorizer\n",
    "\n",
    "* Try most common words across all texts\n",
    "* Bigrams and trigrams are arrangements of two- and three-words, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(analyzer=str.split, \n",
    "                            stop_words='english', \n",
    "                            lowercase=True, \n",
    "                            use_idf=True, \n",
    "                            norm='l2', \n",
    "                            smooth_idf=True, \n",
    "                            max_features=None, \n",
    "                            ngram_range=(1,2), \n",
    "                            max_df=0.6, \n",
    "                            min_df=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextStats(BaseEstimator, TransformerMixin):\n",
    "# Extract features from each document for DictVectorizer\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, sents):\n",
    "        return [{'LENGTH': len(sent), \n",
    "                 'ADJ': sent.count('ADJ'), \n",
    "                 'NOUN': sent.count('NOUN'), \n",
    "                 'VERB': sent.count('VERB'), \n",
    "                 'ADV': sent.count('ADV'), \n",
    "                 'ADP': sent.count('ADP'), \n",
    "                 'CCONJ': sent.count('CCONJ'), \n",
    "                 'INTJ': sent.count('INTJ'), \n",
    "                 'PART': sent.count('PART'), \n",
    "                 'SCONJ': sent.count('SCONJ'), \n",
    "                 'PRON': sent.count('PRON'), \n",
    "                 'PROPN': sent.count('PROPN'), \n",
    "                 'DET': sent.count('DET'), \n",
    "                 'NUM': sent.count('NUM'), \n",
    "                 'AUX': sent.count('AUX'), \n",
    "                 'SYM': sent.count('SYM'), \n",
    "                }\n",
    "                for sent in sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dict Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "# list of dicts -> feature matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent Classifier\n",
    "* Combines support vector classifier with gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd = SGDClassifier(penalty='elasticnet', \n",
    "                    alpha=0.0001, \n",
    "                    class_weight=None, \n",
    "                    loss='log', \n",
    "                    tol=0.005, \n",
    "                    fit_intercept=True, \n",
    "                    power_t=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "* Chains transformers and estimator as input to grid search algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list=[\n",
    "            ('text', Pipeline([\n",
    "                ('selector', ItemSelector(key='text')), \n",
    "                ('vect', vectorizer), \n",
    "            ])), \n",
    "            ('pos', Pipeline([\n",
    "                ('selector', ItemSelector(key='pos')), \n",
    "                ('stats', TextStats()), \n",
    "                ('dict', DictVectorizer()), \n",
    "            ])), \n",
    "        ], \n",
    "        transformer_weights={\n",
    "            'text': 1.0, \n",
    "            'pos': 0.1, \n",
    "        }, \n",
    "    )), \n",
    "    ('model', sgd), \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "### Hyperparameters\n",
    "* List parameters to search over for best cv fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vect_features = [None, 20000, 10000, 5000, 1000]\n",
    "vect_ngrams = [(1,1), (1,2), (1,3)]\n",
    "vect_max = [0.6, 0.7, 0.8]\n",
    "vect_min = [1, 2, 3]\n",
    "vect_norm = ['l2', 'l1', None]\n",
    "vect_stop = ['english', None]\n",
    "model_alpha = [0.001, 0.0001, 0.00001]\n",
    "model_loss = ['hinge', 'log']\n",
    "model_tol = [0.01, 0.005, 0.001]\n",
    "model_class_weight = ['balanced', None]\n",
    "model_power_t = [0.4, 0.5, 0.6]\n",
    "model_penalty = ['l2', 'elasticnet', 'l1', 'none']\n",
    "model_intercept = [True, False]\n",
    "\n",
    "parameters = {\n",
    "#         'union__text__vect__max_features': vect_features, \n",
    "        'union__text__vect__ngram_range': vect_ngrams, \n",
    "#         'union__text__vect__max_df': vect_max, \n",
    "#         'union__text__vect__min_df': vect_min, \n",
    "#         'union__text__vect__norm': vect_norm, \n",
    "#         'union__text__vect__stop_words': vect_stop, \n",
    "#         'model__alpha': model_alpha, \n",
    "        'model__loss': model_loss, \n",
    "#         'model__tol': model_tol,\n",
    "#         'model__class_weight': model_class_weight, \n",
    "#         'model__power_t': model_power_t, \n",
    "#         'model__penalty': model_penalty, \n",
    "#         'model__fit_intercept': model_intercept\n",
    "    }\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid = GridSearchCV(pipe, \n",
    "                    parameters, \n",
    "                    scoring='f1_micro', \n",
    "                    return_train_score=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "\n",
      "Completed in 6.13 seconds\n"
     ]
    }
   ],
   "source": [
    "print('Performing grid search...')\n",
    "t0 = time()\n",
    "grid.fit(X_train, y_train)\n",
    "print('\\nCompleted in {:.2f} seconds'.format(time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Training Set Score: 88.87%\n",
      "\n",
      "Best Cross-Val Score: 66.03%\n",
      "\n",
      "Best Parameters:\n",
      "\n",
      "model__loss: log\n",
      "union__text__vect__ngram_range: (1, 2)\n",
      "\n",
      "Test set score: 67.2%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted Author</th>\n",
       "      <th>Machado</th>\n",
       "      <th>Swirsky</th>\n",
       "      <th>Valente</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual Author</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Machado</th>\n",
       "      <td>215</td>\n",
       "      <td>36</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Swirsky</th>\n",
       "      <td>72</td>\n",
       "      <td>210</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Valente</th>\n",
       "      <td>53</td>\n",
       "      <td>26</td>\n",
       "      <td>348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted Author  Machado  Swirsky  Valente\n",
       "Actual Author                              \n",
       "Machado               215       36       89\n",
       "Swirsky                72      210      102\n",
       "Valente                53       26      348"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('\\nBest Training Set Score: {:.2%}'.format(grid.cv_results_['mean_train_score'][grid.best_index_]))\n",
    "print('\\nBest Cross-Val Score: {:.2%}'.format(grid.best_score_))\n",
    "\n",
    "print('\\nBest Parameters:\\n')\n",
    "best_parameters = grid.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print('{}: {}'.format(param_name, best_parameters[param_name]))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "test_score = grid.score(X_test, y_test)\n",
    "y_test_pred = grid.predict(X_test)\n",
    "test_cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "print('\\nTest set score: {:.1%}'.format(test_score))\n",
    "\n",
    "df_test_cm = pd.DataFrame(test_cm, index=grid.classes_, columns=grid.classes_)\n",
    "df_test_cm.index.name = 'Actual Author'\n",
    "df_test_cm.columns.name = 'Predicted Author'\n",
    "display(df_test_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# grid.best_estimator_.named_steps['union'].transformer_list[0][1].named_steps['vect']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# grid.best_estimator_.named_steps['union'].transformer_list[1][1].named_steps['dict'].get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>param_model__loss</th>\n",
       "      <th>param_union__text__vect__ngram_range</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64.33</td>\n",
       "      <td>0.65</td>\n",
       "      <td>90.37</td>\n",
       "      <td>1.04</td>\n",
       "      <td>0.16</td>\n",
       "      <td>hinge</td>\n",
       "      <td>(1, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>66.03</td>\n",
       "      <td>0.79</td>\n",
       "      <td>88.87</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.15</td>\n",
       "      <td>log</td>\n",
       "      <td>(1, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>63.55</td>\n",
       "      <td>1.14</td>\n",
       "      <td>89.34</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.15</td>\n",
       "      <td>hinge</td>\n",
       "      <td>(1, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>60.14</td>\n",
       "      <td>1.35</td>\n",
       "      <td>80.11</td>\n",
       "      <td>2.49</td>\n",
       "      <td>0.15</td>\n",
       "      <td>log</td>\n",
       "      <td>(1, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>62.51</td>\n",
       "      <td>1.65</td>\n",
       "      <td>87.83</td>\n",
       "      <td>2.23</td>\n",
       "      <td>0.15</td>\n",
       "      <td>hinge</td>\n",
       "      <td>(1, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>63.38</td>\n",
       "      <td>2.28</td>\n",
       "      <td>84.99</td>\n",
       "      <td>4.86</td>\n",
       "      <td>0.16</td>\n",
       "      <td>log</td>\n",
       "      <td>(1, 1)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_test_score  std_test_score  mean_train_score  std_train_score  \\\n",
       "2            64.33            0.65             90.37             1.04   \n",
       "4            66.03            0.79             88.87             0.40   \n",
       "1            63.55            1.14             89.34             0.42   \n",
       "5            60.14            1.35             80.11             2.49   \n",
       "0            62.51            1.65             87.83             2.23   \n",
       "3            63.38            2.28             84.99             4.86   \n",
       "\n",
       "   mean_fit_time param_model__loss param_union__text__vect__ngram_range  \n",
       "2           0.16             hinge                               (1, 3)  \n",
       "4           0.15               log                               (1, 2)  \n",
       "1           0.15             hinge                               (1, 2)  \n",
       "5           0.15               log                               (1, 3)  \n",
       "0           0.15             hinge                               (1, 1)  \n",
       "3           0.16               log                               (1, 1)  "
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results = pd.DataFrame()\n",
    "cv_results['mean_test_score'] = (grid.cv_results_['mean_test_score'] * 100).round(2)\n",
    "cv_results['std_test_score'] = (grid.cv_results_['std_test_score'] * 100).round(2)\n",
    "cv_results['mean_train_score'] = (grid.cv_results_['mean_train_score'] * 100).round(2)\n",
    "cv_results['std_train_score'] = (grid.cv_results_['std_train_score'] * 100).round(2)\n",
    "cv_results['mean_fit_time'] = grid.cv_results_['mean_fit_time'].round(2)\n",
    "cv_results = cv_results.join(pd.DataFrame(grid.cv_results_).filter(like='param_'))\n",
    "cv_results.sort_values('std_test_score', ascending=True, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3556 features\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PROPN         7.96\n",
       "son           4.41\n",
       "cal           4.34\n",
       "ribbon        4.06\n",
       "parents       4.03\n",
       "doctor        3.90\n",
       "girlfriend    3.80\n",
       "rose          3.79\n",
       "says          3.67\n",
       "going         3.48\n",
       "woman         3.44\n",
       "2015          3.12\n",
       "sister        3.09\n",
       "slender       3.09\n",
       "loved         3.02\n",
       "my            2.97\n",
       "students      2.87\n",
       "lion          2.87\n",
       "lions         2.83\n",
       "i             2.81\n",
       "Name: Machado, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "rose        7.27\n",
       "d           4.38\n",
       "lion        4.27\n",
       "love        3.92\n",
       "says        3.86\n",
       "bird        3.44\n",
       "NUM         3.41\n",
       "mother      3.38\n",
       "we          3.37\n",
       "i           3.25\n",
       "lions       3.22\n",
       "ivory       3.16\n",
       "pedestal    3.05\n",
       "must        3.03\n",
       "mine        3.01\n",
       "him         2.92\n",
       "sculptor    2.85\n",
       "story       2.74\n",
       "across      2.58\n",
       "statue      2.56\n",
       "Name: Swirsky, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "rose          5.39\n",
       "lion          5.36\n",
       "lions         4.38\n",
       "PROPN         4.12\n",
       "d             3.58\n",
       "big           3.54\n",
       "machine       3.53\n",
       "sea           3.44\n",
       "dormouse      3.42\n",
       "woman         3.39\n",
       "difference    3.34\n",
       "inside        3.32\n",
       "him           3.22\n",
       "dreambody     2.98\n",
       "means         2.86\n",
       "jewel         2.86\n",
       "smallgod      2.85\n",
       "husband       2.84\n",
       "black         2.83\n",
       "mother        2.69\n",
       "Name: Valente, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_list1 = grid.best_estimator_.named_steps['union'].transformer_list[0][1].named_steps['vect'].get_feature_names()\n",
    "feature_list2 = grid.best_estimator_.named_steps['union'].transformer_list[1][1].named_steps['dict'].get_feature_names()\n",
    "feature_list = feature_list1 + feature_list2\n",
    "feature_coefficients = pd.DataFrame(grid.best_estimator_.steps[-1][1].coef_, columns=feature_list, index=grid.classes_)\n",
    "print('\\n{} features'.format(len(feature_list)))\n",
    "feature_coefficients = feature_coefficients.abs().round(2)\n",
    "display(feature_coefficients.loc['Machado'].sort_values(ascending=False).head(20))\n",
    "display(feature_coefficients.loc['Swirsky'].sort_values(ascending=False).head(20))\n",
    "display(feature_coefficients.loc['Valente'].sort_values(ascending=False).head(20))\n",
    "# display(feature_coefficients.sort_values('Valente', ascending=False, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "264px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
